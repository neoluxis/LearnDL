{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [(0, 255, 0), (255, 0, 0)]\n",
    "CLASSES = ['pool', 'lack_of_fusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2mask(json_file, root='mydata', show=False):\n",
    "    \"\"\"According to the json file, create a mask for the image.\n",
    "\n",
    "    Save mask file with same name in {root}/seg/ folder. \n",
    "\n",
    "    Save into .jpg format.  \n",
    "\n",
    "    Args:\n",
    "        json_file (str): Json file path\n",
    "        show (bool, optional): Show the mask. Defaults to False.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.join(root, 'seg'), exist_ok=True)\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    img = np.zeros(\n",
    "        (data['imageHeight'], data['imageWidth'], 3), dtype=np.uint8)\n",
    "    save_path = os.path.join(root, 'seg', os.path.basename(\n",
    "        json_file).split('.')[0] + '.jpg')\n",
    "    labels = data['shapes']\n",
    "    for label in labels:\n",
    "        points = np.array(label['points'], dtype=np.int32)\n",
    "        cv.fillPoly(img, [points], COLORS[CLASSES.index(label['label'])])\n",
    "    cv.imwrite(save_path, img)\n",
    "    if show:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def convert_img(root='mydata'):\n",
    "    \"\"\"Convert images into .jpg format.\n",
    "\n",
    "    Args:\n",
    "        root (str, optional): _description_. Defaults to 'mydata'.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.join(root, 'Images'), exist_ok=True)\n",
    "    for img_file in tqdm(os.listdir(os.path.join(root, 'bmp'))):\n",
    "        img = cv.imread(os.path.join(root, 'bmp', img_file))\n",
    "        cv.imwrite(os.path.join(\n",
    "            root, 'Images', img_file.split('.')[0] + '.jpg'), img)\n",
    "\n",
    "\n",
    "# for json_file in tqdm(os.listdir('mydata/Json')):\n",
    "#     label2mask(os.path.join('mydata/Json', json_file))\n",
    "# convert_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\" Display a list of images.\n",
    "    \n",
    "    Args:\n",
    "        imgs (list): List of images\n",
    "        num_rows (int): Number of rows\n",
    "        num_cols (int): Number of columns\n",
    "        titles (list, optional): List of titles. Defaults to None.\n",
    "        scale (float, optional): Scale. Defaults to 1.5.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of axes\n",
    "    \"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(root='mydata'):\n",
    "    \"\"\"Read images from root folder.\n",
    "\n",
    "        The root folder should look like this:   \n",
    "\n",
    "        root\n",
    "        ├── Images: images .jpg/.png\n",
    "        ├── Json: json files\n",
    "        ├── seg: mask files .jpg/.png\n",
    "        ├── Annotations: annotations files (Needed to create seg/)\n",
    "        ├── labels: labels files (optional)\n",
    "        ├── classes.txt: classes files  \n",
    "\n",
    "    Args:\n",
    "        root (str, optional): Root folder of images\n",
    "        ratio (list, optional): Ratio of train, val, test.\n",
    "    \"\"\"\n",
    "    mode = torchvision.io.image.ImageReadMode.RGB\n",
    "    img_paths = []\n",
    "    label_paths = []\n",
    "    for img in os.listdir(os.path.join(root, 'Images')):\n",
    "        img_paths.append(os.path.join(root, 'Images', img))\n",
    "        label_paths.append(os.path.join(root, 'seg', img))\n",
    "    print('Total images:', len(img_paths))\n",
    "    print('Total labels:', len(label_paths))\n",
    "    images, labels = [], []\n",
    "    for i, img_path in enumerate(img_paths):\n",
    "        images.append(torchvision.io.read_image(img_path, mode))\n",
    "        labels.append(torchvision.io.read_image(label_paths[i], mode))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def voc_colormap2label():\n",
    "    \"\"\"构建从RGB到VOC类别索引的映射\"\"\"\n",
    "    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
    "    for i, colormap in enumerate(COLORS):\n",
    "        colormap2label[\n",
    "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "    return colormap2label\n",
    "\n",
    "\n",
    "def voc_label_indices(colormap, colormap2label):\n",
    "    \"\"\"将VOC标签中的RGB值映射到它们的类别索引\"\"\"\n",
    "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
    "           + colormap[:, :, 2])\n",
    "    return colormap2label[idx]\n",
    "\n",
    "\n",
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"随机裁剪特征和标签图像\"\"\"\n",
    "    rect = torchvision.transforms.RandomCrop.get_params(\n",
    "        feature, (height, width))\n",
    "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
    "    label = torchvision.transforms.functional.crop(label, *rect)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCSegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"一个用于加载VOC数据集的自定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, crop_size, voc_dir):\n",
    "        self.transform = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        features, labels = read_images(voc_dir)\n",
    "        self.features = [self.normalize_image(feature)\n",
    "                         for feature in self.filter(features)]\n",
    "        self.labels = self.filter(labels)\n",
    "        self.colormap2label = voc_colormap2label()\n",
    "        print('read ' + str(len(self.features)) + ' examples')\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return self.transform(img.float() / 255)\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[1] >= self.crop_size[0] and\n",
    "            img.shape[2] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
    "                                       *self.crop_size)\n",
    "        return (feature, voc_label_indices(label, self.colormap2label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "class SimpleSegDataset(Dataset):\n",
    "    def __init__(self, crop_size, voc_dir):\n",
    "        self.transform = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        self.voc_dir = voc_dir\n",
    "        self.filenames = sorted(os.listdir(os.path.join(voc_dir, 'images')))\n",
    "        self.labels_path = os.path.join(voc_dir, 'labels')\n",
    "        self.colormap2label = voc_colormap2label()\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return self.transform(img.float() / 255)\n",
    "\n",
    "    def filter(self, img):\n",
    "        return img.shape[1] >= self.crop_size[0] and img.shape[2] >= self.crop_size[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 这里采用懒加载的方式，仅在调用__getitem__时读取文件\n",
    "        img_path = os.path.join(self.voc_dir, 'images', self.filenames[idx])\n",
    "        label_path = os.path.join(\n",
    "            self.labels_path, self.filenames[idx].split('.')[0] + '.png')\n",
    "        feature = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(label_path)\n",
    "        if self.filter(feature):\n",
    "            feature = self.normalize_image(transforms.ToTensor()(feature))\n",
    "            # 随机裁剪\n",
    "            feature, label = voc_rand_crop(feature, label, *self.crop_size)\n",
    "            # 将标签转化为索引\n",
    "            label = voc_label_indices(label, self.colormap2label)\n",
    "            return feature, label\n",
    "        else:\n",
    "            # 如果图片不符合要求，则返回一个特殊标记，例如None或抛出异常\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\proj\\OverallNN\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\proj\\OverallNN\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "pretrained_net = torchvision.models.resnet18(pretrained=True)\n",
    "net = nn.Sequential(*list(pretrained_net.children())[:-2])\n",
    "\n",
    "num_classes = 2\n",
    "net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n",
    "                                                    kernel_size=64, padding=16, stride=32))\n",
    "\n",
    "\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight\n",
    "\n",
    "\n",
    "conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\n",
    "                                bias=False)\n",
    "conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4))\n",
    "\n",
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "net.transpose_conv.weight.data.copy_(W)\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleSegDataset((512,512), 'mydata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = [int(0.75 * len(dataset)), int(0.15 * len(dataset)), len(dataset) - int(0.75 * len(dataset)) - int(0.15 * len(dataset))]\n",
    "trainset, valset, testset = torch.utils.data.random_split(dataset, ratio)\n",
    "\n",
    "train_iter = DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "val_iter = DataLoader(valset, batch_size=16)\n",
    "test_iter = DataLoader(testset, batch_size=16)\n",
    "\n",
    "len(train_iter), len(val_iter), len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, train_iter, val_iter, num_epochs, optim, loss, device):\n",
    "    train_loss, valid_loss = [], []\n",
    "    train_acc, valid_acc = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        acc_sum, n = 0.0, 0\n",
    "        for X, y in tqdm(train_iter):\n",
    "            optim.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).mean()\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            running_loss += l.item()\n",
    "            acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.numel()\n",
    "        train_loss.append(running_loss / len(train_iter))\n",
    "        train_acc.append(acc_sum / n)\n",
    "        net.eval()\n",
    "        acc_sum, n = 0.0, 0\n",
    "        for X, y in val_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).mean()\n",
    "            acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.numel()\n",
    "        valid_loss.append(l.item())\n",
    "        valid_acc.append(acc_sum / n)\n",
    "\n",
    "        print(\n",
    "            f'epoch: {epoch}, train_loss: {train_loss[-1]:.4f}, valid_loss: {valid_loss[-1]:.4f}, train_acc: {train_acc[-1]:.4f}, valid_acc: {valid_acc[-1]:.4f}')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='train')\n",
    "    plt.plot(valid_loss, label='valid')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='train')\n",
    "    plt.plot(valid_acc, label='valid')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_model(net, train_iter, val_iter, num_epochs, optimizer, loss, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to('cpu')\n",
    "\n",
    "def predict(img, net):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        X = img.unsqueeze(0)\n",
    "        pred = net(X.to(device)).argmax(axis=1)\n",
    "    return pred.reshape(pred.shape[1], pred.shape[2])\n",
    "\n",
    "def label2image(pred):\n",
    "    colormap = torch.tensor(COLORS_RGB, device=device)\n",
    "    X = pred.long()\n",
    "    return colormap[X]\n",
    "\n",
    "img, label = testset[0]\n",
    "pred = predict(img, net)\n",
    "pred = label2image(pred)\n",
    "img = img.cpu()\n",
    "label = label2image(label)\n",
    "label = label.cpu()\n",
    "pred = pred.cpu()\n",
    "show_images([img.permute(1, 2, 0), label2image(label), pred], 1, 3, titles=[\n",
    "    'input', 'label', 'pred'], scale=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
