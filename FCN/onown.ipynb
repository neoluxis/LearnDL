{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2 as cv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "batch_size = 32\n",
    "img_size = [512, 512]\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "colors = [(0, 255, 0), (255, 0, 0)]\n",
    "classes = ['pool', 'lack_of_fusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\proj\\OverallNN\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\proj\\OverallNN\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 512, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_net = torchvision.models.resnet18(pretrained=True)\n",
    "net = nn.Sequential(*list(pretrained_net.children())[:-2])\n",
    "net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n",
    "                                    kernel_size=64, padding=16, stride=32))\n",
    "\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight\n",
    "\n",
    "conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\n",
    "                                bias=False)\n",
    "conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4));\n",
    "\n",
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "net.transpose_conv.weight.data.copy_(W);\n",
    "\n",
    "X = torch.randn(size=(1, 3, 512, 512))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\" Display a list of images.\n",
    "    \n",
    "    Args:\n",
    "        imgs (list): List of images\n",
    "        num_rows (int): Number of rows\n",
    "        num_cols (int): Number of columns\n",
    "        titles (list, optional): List of titles. Defaults to None.\n",
    "        scale (float, optional): Scale. Defaults to 1.5.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of axes\n",
    "    \"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and Read Dataset\n",
    "\n",
    "Dataset folder structure is like below:\n",
    "\n",
    "```\n",
    "Dataset Root\n",
    "    ├── bmp: .bmp images. Original images\n",
    "    ├── jsons: .json files. Labels\n",
    "    ├── images (Optional): Convert .bmp images to .jpg images and save them here\n",
    "    ├── segm (Optional): .png images. Segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2mask(json_file, root='mydata', show=False):\n",
    "    os.makedirs(os.path.join(root, 'segm'), exist_ok=True)\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    img = np.zeros(\n",
    "        (data['imageHeight'], data['imageWidth'], 3), dtype=np.uint8)\n",
    "    save_path = os.path.join(root, 'segm', os.path.basename(\n",
    "        json_file).split('.')[0] + '.png')\n",
    "    labels = data['shapes']\n",
    "    for label in labels:\n",
    "        points = np.array(label['points'], dtype=np.int32)\n",
    "        cv.fillPoly(img, [points], colors[classes.index(label['label'])])\n",
    "    cv.imwrite(save_path, img)\n",
    "    if show:\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "def convert_img(img_path, root='mydata', show=False):\n",
    "    os.makedirs(os.path.join(root, 'images'), exist_ok=True)\n",
    "    img = cv.imread(img_path)\n",
    "    save_path = os.path.join(root, 'images', os.path.basename(img_path).split('.')[0] + '.jpg')\n",
    "    cv.imwrite(save_path, img)\n",
    "    if show:\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlx_rand_crop(feature, label, height=360, width=360):\n",
    "    \"\"\"随机裁剪特征和标签图像\"\"\"\n",
    "    rect = torchvision.transforms.RandomCrop.get_params(\n",
    "        feature, (height, width))\n",
    "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
    "    label = torchvision.transforms.functional.crop(label, *rect)\n",
    "    return feature, label\n",
    "\n",
    "\n",
    "def nlx_color2label():\n",
    "    color2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
    "    for i, color in enumerate(colors):\n",
    "        color2label[(color[0] * 256 + color[1]) * 256 + color[2]] = i\n",
    "    return color2label\n",
    "\n",
    "\n",
    "def nlx_label_indices(colormap, colormap2label):\n",
    "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256 +\n",
    "           colormap[:, :, 2])\n",
    "    return colormap2label[idx]\n",
    "\n",
    "\n",
    "def imgread(path):\n",
    "    img = cv.imread(path)\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def imgwrite(img, path):\n",
    "    img = cv.cvtColor(img, cv.COLOR_RGB2BGR)\n",
    "    cv.imwrite(path, img)\n",
    "\n",
    "\n",
    "def list_imgs(root='mydata', ratio=[0.6, 0.2, 0.2]):\n",
    "    all_imgs = [x.split('.')[0]\n",
    "                for x in os.listdir(os.path.join(root, 'images'))]\n",
    "    train_imgs = all_imgs[:int(ratio[0] * len(all_imgs))]\n",
    "    val_imgs = all_imgs[int(ratio[0] * len(all_imgs)):int(\n",
    "        (ratio[0] + ratio[1]) * len(all_imgs))]\n",
    "    test_imgs = all_imgs[int((ratio[0] + ratio[1]) * len(all_imgs)):]\n",
    "    return train_imgs, val_imgs, test_imgs\n",
    "\n",
    "def read_imgs(root='mydata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLX2024Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, crop_size):\n",
    "        self.transform = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        train_imgs, val_imgs, test_imgs = list_imgs(root)\n",
    "        self.train_imgs = train_imgs\n",
    "        self.val_imgs = val_imgs\n",
    "        self.test_imgs = test_imgs\n",
    "        self.color2label = nlx_color2label()\n",
    "        print('read ' + str(len(self.train_imgs) +\n",
    "              len(self.val_imgs) + len(self.test_imgs)) + ' images')\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
