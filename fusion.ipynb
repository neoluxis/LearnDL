{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class color (BGR)\n",
    "COLORS_BGR = [(0, 255, 0), (0, 0, 255)]\n",
    "COLORS_RGB = [(0, 255, 0), (255, 0, 0)]\n",
    "CLASSES = ['pool', 'lack_of_fusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2mask(json_file, root='mydata', show=False):\n",
    "    \"\"\"According to the json file, create a mask for the image.\n",
    "\n",
    "    Save mask file with same name in {root}/seg/ folder. \n",
    "\n",
    "    Save into .jpg format.  \n",
    "\n",
    "    Args:\n",
    "        json_file (str): Json file path\n",
    "        show (bool, optional): Show the mask. Defaults to False.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.join(root, 'seg'), exist_ok=True)\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    img = np.zeros(\n",
    "        (data['imageHeight'], data['imageWidth'], 3), dtype=np.uint8)\n",
    "    save_path = os.path.join(root, 'seg', os.path.basename(\n",
    "        json_file).split('.')[0] + '.jpg')\n",
    "    labels = data['shapes']\n",
    "    for label in labels:\n",
    "        points = np.array(label['points'], dtype=np.int32)\n",
    "        cv.fillPoly(img, [points], COLORS_BGR[CLASSES.index(label['label'])])\n",
    "    cv.imwrite(save_path, img)\n",
    "    if show:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def convert_img(root='mydata'):\n",
    "    \"\"\"Convert images into .jpg format.\n",
    "\n",
    "    Args:\n",
    "        root (str, optional): _description_. Defaults to 'mydata'.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.join(root, 'Images'), exist_ok=True)\n",
    "    for img_file in tqdm(os.listdir(os.path.join(root, 'bmp'))):\n",
    "        img = cv.imread(os.path.join(root, 'bmp', img_file))\n",
    "        cv.imwrite(os.path.join(\n",
    "            root, 'Images', img_file.split('.')[0] + '.jpg'), img)\n",
    "\n",
    "\n",
    "# for json_file in tqdm(os.listdir('mydata/Json')):\n",
    "#     label2mask(os.path.join('mydata/Json', json_file))\n",
    "# convert_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\" Display a list of images.\n",
    "    \n",
    "    Args:\n",
    "        imgs (list): List of images\n",
    "        num_rows (int): Number of rows\n",
    "        num_cols (int): Number of columns\n",
    "        titles (list, optional): List of titles. Defaults to None.\n",
    "        scale (float, optional): Scale. Defaults to 1.5.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of axes\n",
    "    \"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(root='mydata'):\n",
    "    \"\"\"Read images from root folder.\n",
    "\n",
    "        The root folder should look like this:   \n",
    "\n",
    "        root\n",
    "        ├── Images: images .jpg/.png\n",
    "        ├── Json: json files\n",
    "        ├── seg: mask files .jpg/.png\n",
    "        ├── Annotations: annotations files (Needed to create seg/)\n",
    "        ├── labels: labels files (optional)\n",
    "        ├── classes.txt: classes files  \n",
    "\n",
    "    Args:\n",
    "        root (str, optional): Root folder of images\n",
    "        ratio (list, optional): Ratio of train, val, test.\n",
    "    \"\"\"\n",
    "    mode = torchvision.io.image.ImageReadMode.RGB\n",
    "    img_paths = []\n",
    "    label_paths = []\n",
    "    for img in os.listdir(os.path.join(root, 'Images')):\n",
    "        img_paths.append(os.path.join(root, 'Images', img))\n",
    "        label_paths.append(os.path.join(root, 'seg', img))\n",
    "    print('Total images:', len(img_paths))\n",
    "    print('Total labels:', len(label_paths))\n",
    "    images, labels = [], []\n",
    "    for i, img_path in enumerate(img_paths):\n",
    "        images.append(torchvision.io.read_image(img_path, mode))\n",
    "        labels.append(torchvision.io.read_image(label_paths[i], mode))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def voc_colormap2label():\n",
    "    \"\"\"构建从RGB到VOC类别索引的映射\"\"\"\n",
    "    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
    "    for i, colormap in enumerate(COLORS_RGB):\n",
    "        colormap2label[\n",
    "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "    return colormap2label\n",
    "\n",
    "\n",
    "def voc_label_indices(colormap, colormap2label):\n",
    "    \"\"\"将VOC标签中的RGB值映射到它们的类别索引\"\"\"\n",
    "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
    "           + colormap[:, :, 2])\n",
    "    return colormap2label[idx]\n",
    "\n",
    "\n",
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"随机裁剪特征和标签图像\"\"\"\n",
    "    rect = torchvision.transforms.RandomCrop.get_params(\n",
    "        feature, (height, width))\n",
    "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
    "    label = torchvision.transforms.functional.crop(label, *rect)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCSegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"一个用于加载VOC数据集的自定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, crop_size, voc_dir):\n",
    "        self.transform = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        features, labels = read_images(voc_dir)\n",
    "        self.features = [self.normalize_image(feature)\n",
    "                         for feature in self.filter(features)]\n",
    "        self.labels = self.filter(labels)\n",
    "        self.colormap2label = voc_colormap2label()\n",
    "        print('read ' + str(len(self.features)) + ' examples')\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return self.transform(img.float() / 255)\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[1] >= self.crop_size[0] and\n",
    "            img.shape[2] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
    "                                       *self.crop_size)\n",
    "        return (feature, voc_label_indices(label, self.colormap2label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "pretrained_net = torchvision.models.resnet18(pretrained=True)\n",
    "net = nn.Sequential(*list(pretrained_net.children())[:-2])\n",
    "\n",
    "num_classes = 21\n",
    "net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n",
    "                                                    kernel_size=64, padding=16, stride=32))\n",
    "\n",
    "\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight\n",
    "\n",
    "\n",
    "conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\n",
    "                                bias=False)\n",
    "conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4))\n",
    "\n",
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "net.transpose_conv.weight.data.copy_(W)\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VOCSegDataset((512,512), 'mydata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
